warming up the model with an empty run

llama server listening at http://127.0.0.1:9090

